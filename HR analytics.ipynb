{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HR analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">It is also known as people analytics.\n",
    ">Is a data driven approach to manage peple at work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems addressed by HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> decisions related to employee hiring and retention.\n",
    "> performance evaluation\n",
    "> collaboration etc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this project we will cncentrate predicting employee turnover :Hiring and Retention\n",
    "Employee turnover is the process of employees leaving the company also known as employee attrition or churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When skilled employees leave the company this can be very costly for the company thus firms are interested in predicting turnover before hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Having that information in hand companies can change their strategies to retain good workers or start the hiring process of new employees on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"E:\\\\aegis\\\\HRanalytics\\\\turnover.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 10 columns):\n",
      "satisfaction            14999 non-null float64\n",
      "evaluation              14999 non-null float64\n",
      "number_of_projects      14999 non-null int64\n",
      "average_montly_hours    14999 non-null int64\n",
      "time_spend_company      14999 non-null int64\n",
      "work_accident           14999 non-null int64\n",
      "churn                   14999 non-null int64\n",
      "promotion               14999 non-null int64\n",
      "department              14999 non-null object\n",
      "salary                  14999 non-null object\n",
      "dtypes: float64(2), int64(6), object(2)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding categorical variables\n",
    "Categorical variables are variables that receive a limited number of values that describe a category. They can be of two types:\n",
    "\n",
    "Ordinal – variables with two or more categories that can be ranked or ordered (e.g. “low”, “medium”, “high”)\n",
    "Nominal – variables with two or more categories that do not have an intrinsic order (e.g. “men”, “women”)\n",
    "In this exercise, you will find the categorical variables in the dataset. To do that, first of all, you will import the pandas library and read the CSV file called \"turnover.csv\". Then, after viewing the first 5 rows and learning (visually) that there are non-numeric values in the DataFrame, you will get some information about the types of variables that are available in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>number_of_projects</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>work_accident</th>\n",
       "      <th>churn</th>\n",
       "      <th>promotion</th>\n",
       "      <th>department</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction  evaluation  number_of_projects  average_montly_hours  \\\n",
       "0          0.38        0.53                   2                   157   \n",
       "1          0.80        0.86                   5                   262   \n",
       "2          0.11        0.88                   7                   272   \n",
       "3          0.72        0.87                   5                   223   \n",
       "4          0.37        0.52                   2                   159   \n",
       "\n",
       "   time_spend_company  work_accident  churn  promotion department  salary  \n",
       "0                   3              0      1          0      sales     low  \n",
       "1                   6              0      1          0      sales  medium  \n",
       "2                   4              0      1          0      sales  medium  \n",
       "3                   5              0      1          0      sales     low  \n",
       "4                   3              0      1          0      sales     low  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing categoricals\n",
    "Remember from the previous exercise that:\n",
    "\n",
    "Ordinal variables have two or more categories which can be ranked or ordered\n",
    "Nominal variables have two or more categories which do not have an intrinsic order\n",
    "In your dataset:\n",
    "\n",
    "salary is an ordinal variable\n",
    "department is a nominal variable\n",
    "In this exercise, you're going to observe the categorical variables found in the previous exercise. To do that, first of all, you will import the pandas library and read the CSV file called \"turnover.csv\". Then, you will print the unique values of those variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sales' 'accounting' 'hr' 'technical' 'support' 'management' 'IT'\n",
      " 'product_mng' 'marketing' 'RandD']\n",
      "['low' 'medium' 'high']\n"
     ]
    }
   ],
   "source": [
    "# Print the unique values of the \"department\" column\n",
    "print(data.department.unique())\n",
    "\n",
    "# Print the unique values of the \"salary\" column\n",
    "print(data.salary.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categories"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You need to help your algorithm understand that you're dealing with categories. You will encode categories of the salary variable, which you know is ordinal based on the values you observed:\n",
    "\n",
    "you first have to tell Python that the salary column is actually categorical\n",
    "you then have to specify the correct order of categories\n",
    "finally, you should encode each category with a numeric value corresponding to its specific position in the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the type of the \"salary\" column to categorical\n",
    "data.salary = data.salary.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the correct order of categories\n",
    "data.salary = data.salary.cat.reorder_categories(['low', 'medium', 'high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categories\n",
    "data.salary = data.salary.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our salary column is now encoded as an ordered category, and optimized for our prediction algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting dummies\n",
    "You will now transform the department variable, which you know is nominal based on the values you observed. To do that, you will use so-called dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies and save them inside a new DataFrame\n",
    "departments = pd.get_dummies(data.department)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IT  RandD  accounting  hr  management  marketing  product_mng  sales  \\\n",
      "0   0      0           0   0           0          0            0      1   \n",
      "1   0      0           0   0           0          0            0      1   \n",
      "2   0      0           0   0           0          0            0      1   \n",
      "3   0      0           0   0           0          0            0      1   \n",
      "4   0      0           0   0           0          0            0      1   \n",
      "\n",
      "   support  technical  \n",
      "0        0          0  \n",
      "1        0          0  \n",
      "2        0          0  \n",
      "3        0          0  \n",
      "4        0          0  \n"
     ]
    }
   ],
   "source": [
    "# Take a quick look to the first 5 rows of the new DataFrame called departments\n",
    "print(departments.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy trap\n",
    "A dummy trap is a situation where different dummy variables convey the same information. In this case, if an employee is, say, from the accounting department (i.e. value in the accounting column is 1), then you're certain that s/he is not from any other department (values everywhere else are 0). Thus, you could actually learn about his/her department by looking at all the other departments.\n",
    "\n",
    "For that reason, whenever n dummies are created (in your case, 10), only n - 1 (in your case, 9) of them are enough, and the n-th column's information is already included.\n",
    "\n",
    "Therefore, you will get rid of the old department column, drop one of the department dummies to avoid dummy trap, and then the two DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"accounting\" column to avoid \"dummy trap\"\n",
    "departments = departments.drop(\"accounting\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the old column \"department\" as you don't need it anymore\n",
    "data = data.drop(\"department\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14999, 9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the new dataframe \"departments\" to your employee dataset: done\n",
    "data = data.join(departments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14999, 18)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage of employees who churn\n",
    "The column churn is providing information about whether an employee has left the company or not is the column churn:\n",
    "\n",
    "if the value of this column is 0, the employee is still with the company\n",
    "if the value of this column is 1, then the employee has left the company\n",
    "Let’s calculate the turnover rate:\n",
    "\n",
    "you will first count the number of times the variable churn has the value 1 and the value 0, respectively\n",
    "you will then divide both counts by the total, and multiply the result by 100 to get the percentage of employees who left and stayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    11428\n",
      "1     3571\n",
      "Name: churn, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of observations and save it as the number of employees\n",
    "n_employees = len(data)\n",
    "\n",
    "# Print the number of employees who left/stayed\n",
    "print(data.churn.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    76.191746\n",
      "1    23.808254\n",
      "Name: churn, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print the percentage of employees who left/stayed\n",
    "print(data.churn.value_counts()/n_employees*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As you can see, 11,428 employees stayed, which accounts for about 76% of the total employee count. Similarly, 3,571 employees left, which accounts for about 24% of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating Target and Features\n",
    "In order to make a prediction (in this case, whether an employee would leave or not), one needs to separate the dataset into two components:\n",
    "\n",
    "the dependent variable or target which needs to be predicted\n",
    "the independent variables or features that will be used to make a prediction\n",
    "Your task is to separate the target and features. The target you have here is the employee churn, and features include everything else.\n",
    "\n",
    "Reminder: the dataset has already been modified by encoding categorical variables and getting dummies.\n",
    "\n",
    "pandas has been imported for you as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the dependent variable column (churn) and set it as target\n",
    "target = data.churn\n",
    "\n",
    "# Drop column churn and set everything else as features\n",
    "features = data.drop(\"churn\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting employee data\n",
    "Overfitting the dataset is a common problem in analytics. This happens when a model is working well on the dataset it was developed upon, but fails to generalize outside of it.\n",
    "\n",
    "A train/test split is implemented to ensure model generalization: you develop the model using the training sample and try it out on the test sample later on.\n",
    "\n",
    "In this exercise, you will split both target and features into train and test sets with 75%/25% ratio, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the function for splitting dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use that function to create the splits both for target and for features\n",
    "# Set the test sample to be 25% of your observations\n",
    "target_train, target_test, features_train, features_test = train_test_split(target,features,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Gini index\n",
    "The decision tree algorithm aims to achieve partitions in the terminal nodes that are as pure as possible. The Gini index is one of the methods used to achieve this. It is calculated based on the proportion of samples in each group.\n",
    "\n",
    "Given the number of people who stayed and left respectively, calculate the Gini index for that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of people who stayed/left\n",
    "stayed = 37\n",
    "left = 1138\n",
    "\n",
    "#sum of stayed and left\n",
    "total = stayed + left\n",
    "\n",
    "#gini index\n",
    "gini = 2*(stayed/total)*(left/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the tree\n",
    "Given the Gini index that would result from splitting by either variable A or B, respectively, decide by which variable the tree should split next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split by B!\n"
     ]
    }
   ],
   "source": [
    "# Gini index in case of splitting by variable A or B\n",
    "gini_A = 0.65\n",
    "gini_B = 0.15\n",
    "\n",
    "# check which Gini is lower and use it for splitting\n",
    "if gini_A < gini_B:\n",
    "    print(\"split by A!\")\n",
    "else:\n",
    "    print(\"split by B!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the tree to employee data\n",
    "A train/test split provides the opportunity to develop the classifier on the training component and test it on the rest of the dataset. In this exercise, you will start developing an employee turnover prediction model using the decision tree classification algorithm. The algorithm provides a .fit() method, which can be used to fit the features to the model in the training set.\n",
    "\n",
    "Reminder: both target and features are already split into train and test components (Train: features_train, target_train, Test: features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the classification algorithm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize it and call model by specifying the random_state parameter\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Apply a decision tree model to fit features to the target\n",
    "model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the accuracy of prediction\n",
    "It’s now time to check how well your trained model can make predictions! Let’s use your testing set to check the accuracy of your Decision Tree model, with the score() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the accuracy score of the prediction for the training set\n",
    "model.score(features_train,target_train)*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.22666666666666"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the accuracy score of the prediction for the test set\n",
    "model.score(features_test,target_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As expected, your algorithm did perfectly on the training set. On the testing set, it was able to correctly predict if an employee would leave or not in almost 98% of the cases! Excellent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the tree\n",
    "In Decision Tree classification tasks, overfitting is usually the result of deeply grown trees. As the comparison of accuracy scores on the train and test sets shows, you have overfitting in your results. This can also be learned from the tree visualization.\n",
    "\n",
    "In this exercise, you will export the decision tree into a text document, which can then be used for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tree graphical visualization export function\n",
    "from sklearn.tree import export_graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply Decision Tree model to fit Features to the Target\n",
    "model.fit(features_train,target_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the tree to a dot file\n",
    "export_graphviz(model,\"tree.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now you can copy the content of tree.dot file and preview it in webgraphviz.com!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning the tree\n",
    "Overfitting is a classic problem in analytics, especially for the decision tree algorithm. Once the tree is fully grown, it may provide highly accurate predictions for the training sample, yet fail to be that accurate on the test set. For that reason, the growth of the decision tree is usually controlled by:\n",
    "\n",
    "“Pruning” the tree and setting a limit on the maximum depth it can have.\n",
    "Limiting the minimum number of observations in one leaf of the tree.\n",
    "In this exercise, you will:\n",
    "\n",
    "prune the tree and limit the growth of the tree to 5 levels of depth\n",
    "fit it to the employee data\n",
    "test prediction results on both training and testing sets.\n",
    "The variables features_train, target_train, features_test and target_test are already available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the DecisionTreeClassifier while limiting the depth of the tree to 5\n",
    "model_depth_5 = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "# Fit and print the model\n",
    "model_depth_5.fit(features_train,target_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.71535247577563\n",
      "97.06666666666666\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy of the prediction for the training set\n",
    "print(model_depth_5.score(features_train,target_train)*100)\n",
    "\n",
    "# Print the accuracy of the prediction for the test set\n",
    "print(model_depth_5.score(features_test,target_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now you have a more reasonable model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limiting the sample size\n",
    "Another method to prevent overfitting is to specify the minimum number of observations necessary to grow a leaf (or node), in the Decision Tree.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "set this minimum limit to 100\n",
    "fit the new model to the employee data\n",
    "examine prediction results on both training and test sets\n",
    "The variables features_train, target_train, features_test and target_test are already available in your workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating accuracy metrics: precision\n",
    "The Precision score is an important metric used to measure the accuracy of a classification algorithm. It is calculated as the fraction of True Positives over the sum of True Positives and False Positives, or\n",
    "# of True Positives# of True Positives+# of False Positives.\n",
    "we define True Positives as the number of employees who actually left, and were classified correctly as leaving\n",
    "we define False Positives as the number of employees who actually stayed, but were wrongly classified as leaving\n",
    "If there are no False Positives, the precision score is equal to 1. If there are no True Positives, the recall score is equal to 0.\n",
    "\n",
    "In this exercise, we will calculate the precision score (using the sklearn function precision_score) for our initial classification model.\n",
    "\n",
    "The variables features_test and target_test are available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9240641711229947"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the function to calculate precision score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Predict whether employees will churn using the test set\n",
    "prediction = model.predict(features_test)\n",
    "\n",
    "# Calculate precision score by comparing target_test with the prediction\n",
    "precision_score(target_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating accuracy metrics: recall\n",
    "The Recall score is another important metric used to measure the accuracy of a classification algorithm. It is calculated as the** fraction of True Positives over the sum of True Positives and False Negatives**, or\n",
    "# of True Positives# of True Positives+# of False Negatives.\n",
    "If there are no False Negatives, the recall score is equal to 1. If there are no True Positives, the recall score is equal to 0.\n",
    "\n",
    "In this exercise, you will calculate the precision score (using the sklearn function recall_score) for your initial classification model.\n",
    "\n",
    "The variables features_test and target_test are available in your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9632107023411371"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the function to calculate recall score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Use the initial model to predict churn\n",
    "prediction = model.predict(features_test)\n",
    "\n",
    "# Calculate recall score by comparing target_test with the prediction\n",
    "recall_score(target_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As the objective here is to develop a model that will correctly predict churn,recall score seems to be our target"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "However recall alone is not enough as by only targeting one class,we may have dramatically low accuracy for the other.\n",
    "Thus a general rule is to use a measure that is not concentrated on one class alone.\n",
    "If your target is have good predictions on both then probably the best choice is to use auc score.\n",
    "it is basically the compound measure that is maximized when both recall and specificity are maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the ROC/AUC score\n",
    "While the Recall score is an important metric for measuring the accuracy of a classification algorithm, it puts too much weight on the number of False Negatives. On the other hand, Precision is concentrated on the number of False Positives.\n",
    "\n",
    "The combination of those two results in the ROC curve allows us to measure both recall and precision. The area under the ROC curve is calculated as the AUC score.\n",
    "\n",
    "In this exercise, you will calculate the ROC/AUC score for the initial model using the sklearn roc_auc_score() function.\n",
    "\n",
    "The variables features_test and target_test are available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9691623087590718"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the function to calculate ROC/AUC score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Use initial model to predict churn (based on features of the test set)\n",
    "prediction = model.predict(features_test)\n",
    "\n",
    "# Calculate ROC/AUC score by comparing target_test with the prediction\n",
    "roc_auc_score(target_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing classes\n",
    "It can significantly affect prediction results, as shown by the difference between the recall and accuracy scores. To solve the imbalance, equal weights are usually given to each class. Using the class_weight argument in sklearn's DecisionTreeClassifier, one can make the classes become \"balanced\".\n",
    "\n",
    "Let’s correct our model by solving its imbalance problem:\n",
    "\n",
    "first, you’re going to set up a model with balanced classes\n",
    "then, you will fit it to the training data\n",
    "finally, you will check its accuracy on the test set\n",
    "The variables features_train, target_train,featurestestandtargettest` are already available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.70666666666668\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DecisionTreeClassifier \n",
    "model_depth_5_b = DecisionTreeClassifier(max_depth=5,class_weight=\"balanced\",random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model_depth_5_b.fit(features_train,target_train)\n",
    "\n",
    "# Print the accuracy of the prediction (in percentage points) for the test set\n",
    "print(model_depth_5_b.score(features_test,target_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation using sklearn\n",
    "As explained in Chapter 2, overfitting the dataset is a common problem in analytics. This happens when a model has learned the data too closely: it has great performances on the dataset it was trained on, but fails to generalize outside of it.\n",
    "\n",
    "While the train/test split technique you learned in Chapter 2 ensures that the model does not overfit the training set, hyperparameter tuning may result in overfitting the test component, since it consists in tuning the model to get the best prediction results on the test set. Therefore, it is recommended to validate the model on different testing sets. K-fold cross-validation allows us to achieve this:\n",
    "\n",
    "it splits the dataset into a training set and a testing set\n",
    "it fits the model, makes predictions and calculates a score (you can specify if you want the accuracy, precision, recall...)\n",
    "it repeats the process k times in total\n",
    "it outputs the average of the 10 scores\n",
    "In this exercise, you will use Cross Validation on our dataset, and evaluate our results with the cross_val_score functio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9853431  0.98533333 0.974      0.96533333 0.96       0.97933333\n",
      " 0.99       0.99333333 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Import the function for implementing cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use that function to print the cross validation score for 10 folds\n",
    "print(cross_val_score(model,features,target,cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up GridSearch parameters\n",
    "A hyperparameter is a parameter inside a function. For example, max_depth or min_samples_leaf are hyperparameters of the DecisionTreeClassifier() function. Hyperparameter tuning is the process of testing different values of hyperparameters to find the optimal ones: the one that gives the best predictions according to your objectives. In sklearn, you can use GridSearch to test different combinations of hyperparameters. Even better, you can use GridSearchCV() test different combinations and run cross-validation on them in one function!\n",
    "\n",
    "In this exercise, you are going to prepare the different values you want to test for max_depth and min_samples_leaf. You will then put these in a dictionary, because that’s what is required for GridSearchCV():\n",
    "\n",
    "the dictionary keys will be the hyperparameters names\n",
    "the dictionary values will be the attributes (the hyperparameter values) you want to test\n",
    "Instead of writing all the values manually, you will use the range() function, which allows us to generate values incrementally. For example, range(1, 10, 2) will generate a list containing values ranging from 1 included to 10 not included, by increments of 2. So the final result will be [1, 3, 5, 7, 9].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate values for maximum depth\n",
    "depth = [i for i in range(5,21,1)]\n",
    "\n",
    "# Generate values for minimum sample size\n",
    "samples = [i for i in range(50,500,50)]\n",
    "\n",
    "# Create the dictionary with parameters to be checked\n",
    "parameters = dict(max_depth=depth, min_samples_leaf=samples)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Implementing GridSearch\n",
    "You can now use the sklearn GridSearchCV() function to find the best combination of all of the max_depth and min_samples_leaf values you generated in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ANACONDA\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'min_samples_leaf': 50}\n"
     ]
    }
   ],
   "source": [
    "# import the GridSearchCV function\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# initialize the param_search function using the GridSearchCV function, initial model and parameters above\n",
    "param_search = GridSearchCV(model, parameters)\n",
    "\n",
    "# fit the param_search to the training dataset\n",
    "param_search.fit(features_train, target_train)\n",
    "\n",
    "# print the best parameters found\n",
    "print(param_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Once decision tree is developed,sklearn can easily develop feature importances.\n",
    "Importance is calculated as relative decrease in Gini due to the selected feature.\n",
    "Once the calculation is done for all the features,the values are rescaled to sum uo to 100%.\n",
    "Higher percentage, higher importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting important features\n",
    "Among other things, Decision Trees are very popular because of their interpretability. Many models can provide accurate predictions, but Decision Trees can also quantify the effect of the different features on the target. Here, it can tell you which features have the strongest and weakest impacts on the decision to leave the company. In sklearn, you can get this information by using the feature_importances_ attribute.\n",
    "\n",
    "In this exercise, you're going to get the quantified importance of each feature, save them in a pandas DataFrame (a Pythonic table), and sort them from the most important to the less important. The model_ best Decision Tree Classifier used in the previous exercises is available in your workspace, as well as the features_test and features_train variables.\n",
    "\n",
    "pandas has been imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.28\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DecisionTreeClassifier \n",
    "model_best = DecisionTreeClassifier(max_depth=8,class_weight=\"balanced\",min_samples_leaf=150,random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model_best.fit(features_train,target_train)\n",
    "\n",
    "# Print the accuracy of the prediction (in percentage points) for the test set\n",
    "print(model_best.score(features_test,target_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importances\n",
    "feature_importances = model_best.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.81864951e-01, 8.63953980e-02, 3.28493324e-02, 3.74608048e-02,\n",
       "       3.58989477e-01, 0.00000000e+00, 0.00000000e+00, 4.45919858e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.91914039e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.52528989e-04, 0.00000000e+00,\n",
       "       1.54967392e-03])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of features: done\n",
    "feature_list = list(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results inside a DataFrame using feature_list as an indnex\n",
    "relative_importances = pd.DataFrame(index=feature_list, data=feature_importances, columns=[\"importance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>satisfaction</th>\n",
       "      <td>0.481865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evaluation</th>\n",
       "      <td>0.086395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_projects</th>\n",
       "      <td>0.032849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_montly_hours</th>\n",
       "      <td>0.037461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_spend_company</th>\n",
       "      <td>0.358989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work_accident</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promotion</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>0.000446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IT</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandD</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>management</th>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_mng</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales</th>\n",
       "      <td>0.000153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technical</th>\n",
       "      <td>0.001550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      importance\n",
       "satisfaction            0.481865\n",
       "evaluation              0.086395\n",
       "number_of_projects      0.032849\n",
       "average_montly_hours    0.037461\n",
       "time_spend_company      0.358989\n",
       "work_accident           0.000000\n",
       "promotion               0.000000\n",
       "salary                  0.000446\n",
       "IT                      0.000000\n",
       "RandD                   0.000000\n",
       "hr                      0.000000\n",
       "management              0.000292\n",
       "marketing               0.000000\n",
       "product_mng             0.000000\n",
       "sales                   0.000153\n",
       "support                 0.000000\n",
       "technical               0.001550"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>satisfaction</th>\n",
       "      <td>0.481865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_spend_company</th>\n",
       "      <td>0.358989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evaluation</th>\n",
       "      <td>0.086395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_montly_hours</th>\n",
       "      <td>0.037461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_projects</th>\n",
       "      <td>0.032849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technical</th>\n",
       "      <td>0.001550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>0.000446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>management</th>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales</th>\n",
       "      <td>0.000153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promotion</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work_accident</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandD</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_mng</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IT</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      importance\n",
       "satisfaction            0.481865\n",
       "time_spend_company      0.358989\n",
       "evaluation              0.086395\n",
       "average_montly_hours    0.037461\n",
       "number_of_projects      0.032849\n",
       "technical               0.001550\n",
       "salary                  0.000446\n",
       "management              0.000292\n",
       "sales                   0.000153\n",
       "promotion               0.000000\n",
       "work_accident           0.000000\n",
       "RandD                   0.000000\n",
       "hr                      0.000000\n",
       "marketing               0.000000\n",
       "product_mng             0.000000\n",
       "support                 0.000000\n",
       "IT                      0.000000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the DataFrame to learn most important features\n",
    "relative_importances.sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting important features\n",
    "In this exercise, your task is to select only the most important features that will be used by the final model. Remember, that the relative importances are saved in the column importance of the DataFrame called relative_importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only features with relative importance higher than 1%\n",
    "selected_features = relative_importances[relative_importances.importance>0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list from those features: done\n",
    "selected_list = selected_features.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['satisfaction', 'evaluation', 'number_of_projects',\n",
       "       'average_montly_hours', 'time_spend_company'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform both features_train and features_test components to include only selected features\n",
    "features_train_selected = features_train[selected_list]\n",
    "features_test_selected = features_test[selected_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop and test the best model\n",
    "In Chapter 3, you found out that the following parameters allow you to get better model:\n",
    "\n",
    "max_depth = 8,\n",
    "min_samples_leaf = 150,\n",
    "class_weight = \"balanced\"\n",
    "In this chapter, you discovered that some of the features have a negligible impact. You realized that you could get accurate predictions using just a small number of selected, impactful features and you updated your training and testing set accordingly, creating the variables features_train_selected and features_test_selected.\n",
    "\n",
    "With all this information at your disposal, you're now going to develop the best model for predicting employee turnover and evaluate it using the appropriate metrics.\n",
    "\n",
    "The features_train_selected and features_test_selected variables are available in your workspace, and the recall_score and roc_auc_score functions have been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=8,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=150, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the best model using parameters provided in description\n",
    "model_best = DecisionTreeClassifier(max_depth=8, min_samples_leaf=150, class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Fit the model using only selected features from training set: done\n",
    "model_best.fit(features_train_selected, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.28\n"
     ]
    }
   ],
   "source": [
    "# Make prediction based on selected list of features from test set\n",
    "prediction_best = model_best.predict(features_test_selected)\n",
    "\n",
    "# Print the general accuracy of the model_best\n",
    "print(model_best.score(features_test_selected, target_test) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.75027870680044\n"
     ]
    }
   ],
   "source": [
    "# Print the recall score of the model predictions\n",
    "print(recall_score(target_test, prediction_best) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.07002193314084\n"
     ]
    }
   ],
   "source": [
    "# Print the ROC/AUC score of the model predictions\n",
    "print(roc_auc_score(target_test, prediction_best) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
